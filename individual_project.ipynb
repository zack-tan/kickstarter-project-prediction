{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "failed        10299\n",
       "successful     5386\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import original dataset\n",
    "df_import = pd.read_excel(\"Kickstarter.xlsx\")\n",
    "\n",
    "# Drop occurrences with irrelevant target states\n",
    "df_filtered = df_import[ (df_import.state == \"failed\") | (df_import.state == \"successful\") ]\n",
    "df_filtered.state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (TEST) Fill NaN in category\n",
    "holes = {\"category\": \"None\"}\n",
    "df_filtered = df_filtered.fillna(value=holes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlated predictors\n",
    "correlations = df_import.corr(method='pearson')\n",
    "#correlations.to_csv(\"indiv_proj_corr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usd_pledged | pledged - 0.9539339\n",
    "backers_count | pledged - 0.72921844\n",
    "USD_pledged | backers_count - 0.7602639\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usd_pledged | pledged - 0.9539339\n",
    "backers_count | pledged - 0.72921844\n",
    "USD_pledged | backers_count - 0.7602639\n",
    "Staff_picked | spotlight - 0.34722496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unary variables\n",
    "with open(\"unary_test.txt\", mode = 'w', encoding= \"utf8\") as filewriter:\n",
    "    for col in df_filtered:\n",
    "        values = df_filtered[col].value_counts()\n",
    "        filewriter.write(f\"\\nOccurrences of each unique value in column {col} :\\n {values}\")\n",
    "        filewriter.write(\"\\n\\n\")\n",
    "    filewriter.close()\n",
    "    \n",
    "# disable_communication is unary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude variables:\n",
    "project_id/name - irrelevant, too specific\n",
    "pledged - correlated with usd_pledged, a better option due to standard measure\n",
    "disable_communication : unary\n",
    "currency - irrelevant, same reason as pledged\n",
    "all deadline vars - launch_to_deadline_days is a better predictor \n",
    "all state_changed vars - seems irrelevant as project's status can be changed at anytime by owners or kickstarter for whatever reason\n",
    "all created_at - create_to_launch_days is a better predictor\n",
    "all launched_at - create_to_launch_days is a better predictor\n",
    "static_usd_rate - irrelevant, same reason as pledged\n",
    "spotlight - only TRUE when project is successful. only FALSE when project is failed. Direct correlation with target. \n",
    "name_len/name_len_cleaned - both correlated, doens't seem meaningful to keep\n",
    "blurb_len/blurb_len_cleaned - both correlated, latter MIGHT be useful to keep\n",
    "\n",
    "\n",
    "Worth noting\n",
    "staff_pick - when true, projects higher chance to succeed. when false, projects mixed chance at success. might be good variable\n",
    "category - 1471 null values exist. but definitely worth keeping as a variable. replace with none for NaN values.\n",
    "blurb_len_cleaned - length of blurb text in project description. Might be good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizes a df with z-score and returns it\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def standardizer(in_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    standardizer = StandardScaler()\n",
    "    return standardizer.fit_transform(in_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Creates and runs a RandomForestClassifier with cv to test a provided X and y.\n",
    "\n",
    "Params:\n",
    "    X               - Predictors \n",
    "    y               - Targets\n",
    "    num_trees       - Number of trees to grow. 500 if unspecified\n",
    "    param_tuning    - Optional. If provided, runs GridSearchCV on the provided params\n",
    "\n",
    "Returns: If param_tuning specified, GridSearchCV object. Otherwise, numpy ndarray from cross_val_score\n",
    "'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def random_forest(X: pd.DataFrame, y: pd.DataFrame, num_trees: int = 500, param_tuning: dict = None):\n",
    "    # Can vary n_estimators\n",
    "    randomForest = RandomForestClassifier(random_state=13, n_estimators=num_trees, bootstrap=True, oob_score=True)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=13)\n",
    "\n",
    "    if param_tuning == None:\n",
    "        scores = cross_val_score(randomForest, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        return scores, randomForest\n",
    "    else:\n",
    "        grid_search = GridSearchCV(estimator=randomForest, scoring='accuracy', param_grid=param_tuning, cv=cv, n_jobs=-1)\n",
    "        grid_result = grid_search.fit(X, y)\n",
    "        return grid_result, randomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
